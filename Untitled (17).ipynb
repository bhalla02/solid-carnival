{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f5eed1-58cc-4a50-ba1b-c2fef63a6caf",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups. To use ANOVA reliably, certain assumptions must be met. These assumptions are:\n",
    "\n",
    "Independence of observations: This assumption states that the observations within each group are independent of each other. In other words, the data points in one group should not be influenced by or related to the data points in another group.\n",
    "\n",
    "Normality: The data within each group should follow a normal distribution. This means that when you plot the data within each group, it should roughly resemble a bell-shaped curve. Violations of this assumption may lead to inaccurate results, especially if the sample sizes are small.\n",
    "\n",
    "Homogeneity of variances (homoscedasticity): This assumption requires that the variance of the data within each group is approximately equal across all groups. In other words, the spread or dispersion of the data points should be similar in each group. Violations of this assumption can lead to increased Type I errors (false positives) or decreased power of the test.\n",
    "\n",
    "Examples of violations of these assumptions that could impact the validity of ANOVA results include:\n",
    "\n",
    "Independence: If observations within groups are not independent, such as when measurements on individuals are repeated over time or when data are collected from clusters or related individuals (e.g., siblings), the assumption of independence is violated.\n",
    "\n",
    "Normality: If the data within each group do not follow a normal distribution, it can affect the accuracy of the p-values and confidence intervals calculated by ANOVA. This could occur, for example, if the data are heavily skewed or if outliers are present.\n",
    "\n",
    "Homogeneity of variances: If the variances of the groups are not approximately equal, it can affect the reliability of the F-statistic in ANOVA. For example, if one group has much larger variance than the others, it may disproportionately influence the overall F-statistic, leading to erroneous conclusions.\n",
    "\n",
    "It's important to assess these assumptions before interpreting the results of an ANOVA analysis. If any of these assumptions are violated, alternative methods or adjustments may be necessary, such as using non-parametric tests or transforming the data to meet the assumptions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ac990-81dd-418e-a1b3-8a6808ad3786",
   "metadata": {},
   "source": [
    "Q2\n",
    "ANOVA (Analysis of Variance) can be categorized into three main types based on the number of factors or independent variables involved:\n",
    "\n",
    "One-Way ANOVA: This type of ANOVA is used when there is only one independent variable or factor with two or more levels or groups. It is used to compare the means of three or more independent groups to determine if there are statistically significant differences among them. For example:\n",
    "\n",
    "Comparing the effectiveness of different teaching methods (e.g., traditional lecture, online module, hands-on activity) on student test scores.\n",
    "Assessing the impact of different dosage levels of a drug (e.g., low, medium, high) on patient recovery time.\n",
    "Two-Way ANOVA: Also known as factorial ANOVA, this type of ANOVA involves two independent variables or factors, each with two or more levels or groups. Two-Way ANOVA allows for the examination of the main effects of each independent variable as well as the interaction between them. It is used to determine if there are significant main effects of each factor and whether there is a significant interaction effect between the factors. For example:\n",
    "\n",
    "Assessing the effects of both gender (male, female) and treatment (drug A, drug B, placebo) on blood pressure levels.\n",
    "Examining the influence of both temperature (low, moderate, high) and humidity (low, moderate, high) on plant growth.\n",
    "Repeated Measures ANOVA: This type of ANOVA is used when the same participants or subjects are measured under different conditions or at different time points. It is used to analyze within-subjects or repeated measures designs where each participant is exposed to multiple conditions or treatments. Repeated Measures ANOVA allows for the assessment of changes over time or across different conditions while accounting for the correlation between repeated measurements on the same participant. For example:\n",
    "\n",
    "Evaluating the effectiveness of a new therapy for depression by measuring participants' mood scores before treatment, during treatment, and after treatment.\n",
    "Investigating the effects of different exercise regimens on athletes' performance by measuring their performance levels before training, after one month of training, and after three months of training.\n",
    "Each type of ANOVA is suitable for different study designs and research questions, so it's important to choose the appropriate ANOVA based on the specific characteristics of the data and the experimental design.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60c58e-cecb-4d25-b716-aa9c4d72bc0c",
   "metadata": {},
   "source": [
    "Q3\n",
    "In ANOVA (Analysis of Variance), the partitioning of variance refers to the decomposition of the total variance observed in the data into different components, each of which represents a different source of variation. Understanding this concept is crucial because it allows researchers to gain insights into how different factors contribute to the overall variability in the data and how much of the total variance can be attributed to each factor.\n",
    "\n",
    "The partitioning of variance in ANOVA involves dividing the total variance into three main components:\n",
    "\n",
    "Between-group variance (SSBetween): This component of variance represents the variation between the means of the different groups or levels of the independent variable. It measures the extent to which the means of the groups differ from each other. In other words, it captures the variability in the dependent variable that can be explained by the differences between the groups.\n",
    "\n",
    "Within-group variance (SSWithin): Also known as error variance, this component represents the variation within each group or level of the independent variable. It measures the extent to which individual data points within each group deviate from the group mean. In other words, it captures the variability in the dependent variable that cannot be explained by the differences between the groups.\n",
    "\n",
    "Total variance (SSTotal): This is the overall variability in the dependent variable across all observations in the dataset. It is the sum of the between-group variance and the within-group variance. It represents the total amount of variability in the dependent variable before accounting for any specific effects of the independent variable.\n",
    "\n",
    "The partitioning of variance is important because it provides valuable information about the relative importance of different factors in explaining the variability observed in the data. By comparing the magnitudes of the between-group and within-group variances, researchers can assess the extent to which the independent variable (or variables) under investigation influences the dependent variable. This understanding helps in drawing meaningful conclusions about the relationships between variables and in determining the significance of the effects observed in ANOVA analyses. Additionally, it guides the interpretation of ANOVA results and informs subsequent analyses, such as post-hoc tests or planned comparisons.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef586d30-c848-43b9-8888-df34747d08e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 60.9\n",
      "Explained Sum of Squares (SSE): 0.09999999999999928\n",
      "Residual Sum of Squares (SSR): 60.8\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (dependent variable)\n",
    "Y = np.array([10, 12, 15, 8, 9, 11, 14, 13, 7, 10])\n",
    "\n",
    "# Overall mean of the dependent variable\n",
    "Y_bar = np.mean(Y)\n",
    "\n",
    "# Group means (example: assuming 2 groups)\n",
    "group1_mean = np.mean(Y[:5])  # Assuming 5 observations in group 1\n",
    "group2_mean = np.mean(Y[5:])  # Assuming 5 observations in group 2\n",
    "\n",
    "# Calculate SST\n",
    "SST = np.sum((Y - Y_bar) ** 2)\n",
    "\n",
    "# Calculate SSE\n",
    "SSE = (5 * (group1_mean - Y_bar) ** 2) + (5 * (group2_mean - Y_bar) ** 2)\n",
    "\n",
    "# Calculate SSR\n",
    "SSR = np.sum((Y[:5] - group1_mean) ** 2) + np.sum((Y[5:] - group2_mean) ** 2)\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc73817-058b-4f2b-82bf-5fc54623a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effects:\n",
      "C(A)[T.A2]    5.0\n",
      "C(A)[T.A3]   -1.0\n",
      "C(B)[T.B2]    2.0\n",
      "dtype: float64\n",
      "\n",
      "Interaction Effect:\n",
      "3.552713678800501e-15\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data (replace with your own dataset)\n",
    "data = pd.DataFrame({\n",
    "    'A': ['A1', 'A1', 'A2', 'A2', 'A3', 'A3'],\n",
    "    'B': ['B1', 'B2', 'B1', 'B2', 'B1', 'B2'],\n",
    "    'Y': [10, 12, 15, 8, 9, 11]\n",
    "})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Y ~ C(A) + C(B) + C(A):C(B)', data=data).fit()\n",
    "\n",
    "# Calculate main effects and interaction effect\n",
    "main_effects = model.params[['C(A)[T.A2]', 'C(A)[T.A3]', 'C(B)[T.B2]']]\n",
    "interaction_effect = model.params['C(A)[T.A3]:C(B)[T.B2]']\n",
    "\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "print(\"\\nInteraction Effect:\")\n",
    "print(interaction_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd631c5-cc0b-4d21-ab40-1a22d08cb213",
   "metadata": {},
   "source": [
    "#Q6\n",
    "\n",
    "In a one-way ANOVA, the F-statistic is used to test whether there are statistically significant differences between the means of the groups. The p-value associated with the F-statistic indicates the probability of observing the obtained F-value (or a more extreme value) if the null hypothesis of no difference between the group means is true.\n",
    "\n",
    "Given the obtained F-statistic of 5.23 and a p-value of 0.02:\n",
    "\n",
    "The F-statistic indicates the ratio of the variability between group means to the variability within groups. A larger F-statistic suggests larger differences between the group means relative to the variability within groups.\n",
    "The p-value of 0.02 indicates the probability of observing an F-statistic as extreme as 5.23 or more extreme, assuming that there are no true differences between the group means (i.e., the null hypothesis is true).\n",
    "Interpretation:\n",
    "\n",
    "Since the p-value (0.02) is less than the significance level (typically 0.05), we reject the null hypothesis.\n",
    "Therefore, we conclude that there are statistically significant differences between the means of at least two of the groups.\n",
    "However, we cannot determine from the ANOVA alone which specific groups differ from each other. Post-hoc tests or planned comparisons would be needed to identify the specific pairwise differences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531ce1f-3ecb-4482-ba61-6537fe4cff7e",
   "metadata": {},
   "source": [
    "Q7\n",
    "Handling missing data in a repeated measures ANOVA requires careful consideration, as it can impact the validity and reliability of the results. Here's how you can handle missing data in repeated measures ANOVA and the potential consequences of using different methods:\n",
    "\n",
    "Handling Missing Data:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion): This approach involves excluding any participant with missing data from the analysis. While straightforward, it can lead to reduced power and biased estimates if data are missing systematically.\n",
    "Mean Substitution: Replace missing values with the mean of the observed values for that variable. This approach can artificially reduce the variability in the data and bias the results if missingness is related to the outcome.\n",
    "Regression Imputation: Predict missing values using regression models based on other observed variables. This method can provide more accurate estimates but assumes that the missingness can be predicted by other variables.\n",
    "Multiple Imputation: Generate multiple plausible values for missing data based on the observed data distribution. Analyze each imputed dataset separately and then combine the results using specific rules. This method preserves variability and provides more accurate estimates compared to mean substitution or regression imputation.\n",
    "Consequences of Different Methods:\n",
    "\n",
    "Bias: Mean substitution can introduce bias if missingness is related to the outcome variable. Listwise deletion may also introduce bias if data are missing not at random (MNAR).\n",
    "Loss of Power: Listwise deletion reduces the sample size and statistical power, especially if missingness is related to the outcome or predictor variables.\n",
    "Inflation of Type I Error: Mean substitution can artificially reduce variability, leading to inflated Type I error rates.\n",
    "Underestimation of Variability: Mean substitution and regression imputation can lead to underestimation of the true variability in the data.\n",
    "Incorrect Parameter Estimates: Different methods may yield different estimates of parameters and standard errors, leading to different conclusions about the effects of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1cf1c4-8ac4-47db-8076-528c6426fd3e",
   "metadata": {},
   "source": [
    "Q8\n",
    "Post-hoc tests are used in ANOVA to compare multiple group means after a significant omnibus F-test. These tests help identify specific group differences when the overall ANOVA indicates that at least one group differs significantly from the others. Some common post-hoc tests include:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD):\n",
    "\n",
    "Tukey's HSD test is widely used and is considered conservative in controlling the familywise error rate.\n",
    "It is appropriate when the number of groups is equal and sample sizes are equal or unequal.\n",
    "Tukey's HSD test compares all possible pairs of group means and identifies which pairs differ significantly.\n",
    "Bonferroni Correction:\n",
    "\n",
    "The Bonferroni correction adjusts the significance level for multiple comparisons by dividing the desired alpha level by the number of comparisons.\n",
    "It is conservative and appropriate for controlling the familywise error rate when conducting multiple pairwise comparisons.\n",
    "Sidak Correction:\n",
    "\n",
    "Similar to the Bonferroni correction, the Sidak correction adjusts the significance level for multiple comparisons.\n",
    "It is less conservative than the Bonferroni correction and can be used when conducting multiple pairwise comparisons.\n",
    "Duncan's New Multiple Range Test:\n",
    "\n",
    "Duncan's test is less conservative than Tukey's HSD test and is suitable when sample sizes are unequal.\n",
    "It performs all pairwise comparisons and identifies significant differences between groups.\n",
    "Scheffé's Test:\n",
    "\n",
    "Scheffé's test is conservative but robust and can be used for any combination of group sizes and variances.\n",
    "It controls the familywise error rate and is suitable for situations where assumptions of other post-hoc tests are violated.\n",
    "Games-Howell Test:\n",
    "\n",
    "The Games-Howell test is suitable when group variances are unequal and sample sizes are unequal.\n",
    "It does not assume equal variances and can handle unequal sample sizes effectively.\n",
    "Example situation:\n",
    "Suppose a researcher conducts an experiment to compare the effectiveness of four different teaching methods on student exam scores. After performing a one-way ANOVA, the researcher finds a significant overall difference in mean exam scores between the four teaching methods. To determine which teaching methods are significantly different from each other, the researcher would use a post-hoc test. In this case, if the sample sizes are equal, the researcher might choose Tukey's HSD test for its conservative nature and ability to control the familywise error rate. If the sample sizes are unequal, Duncan's New Multiple Range Test or the Games-Howell test could be more appropriate. The choice of post-hoc test should consider the specific characteristics of the data and the assumptions of each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d410f0-f77a-44e2-b5f0-7b8af717235a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Way ANOVA Results:\n",
      "F-statistic: 7.663868007623673\n",
      "p-value: 0.0006822732311170656\n",
      "The p-value is less than the significance level, so we reject the null hypothesis.\n",
      "There is sufficient evidence to conclude that there are significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "#Q9\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "weight_loss_A = np.random.normal(loc=5, scale=2, size=50)  # Diet A\n",
    "weight_loss_B = np.random.normal(loc=6, scale=2, size=50)  # Diet B\n",
    "weight_loss_C = np.random.normal(loc=4, scale=2, size=50)  # Diet C\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(weight_loss_A, weight_loss_B, weight_loss_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"One-Way ANOVA Results:\")\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The p-value is less than the significance level, so we reject the null hypothesis.\")\n",
    "    print(\"There is sufficient evidence to conclude that there are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"The p-value is greater than or equal to the significance level, so we fail to reject the null hypothesis.\")\n",
    "    print(\"There is not enough evidence to conclude that there are significant differences between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6534dbd-f45c-4b16-9921-6eade60a0823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA Results:\n",
      "                               sum_sq    df           F        PR(>F)\n",
      "C(Software)                898.466667   2.0  179.693333  3.621520e-15\n",
      "C(Experience)               16.133333   1.0    6.453333  1.796097e-02\n",
      "C(Software):C(Experience)   30.866667   2.0    6.173333  6.870049e-03\n",
      "Residual                    60.000000  24.0         NaN           NaN\n",
      "\n",
      "Interpretation:\n",
      "There is a significant main effect of Software (p < 0.05).\n",
      "There is a significant main effect of Experience (p < 0.05).\n",
      "There is a significant interaction effect between Software and Experience (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "#Q10\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "data = {\n",
    "    'Software': ['A'] * 10 + ['B'] * 10 + ['C'] * 10,\n",
    "    'Experience': ['Novice'] * 5 + ['Experienced'] * 5 + ['Novice'] * 5 + ['Experienced'] * 5 + ['Novice'] * 5 + ['Experienced'] * 5,\n",
    "    'Time': [15, 18, 17, 16, 19, 20, 22, 21, 19, 18, 12, 14, 13, 15, 16, 13, 14, 12, 11, 13, 25, 24, 26, 23, 28, 27, 30, 29, 28, 26]\n",
    "}\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=df).fit()\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA table\n",
    "print(\"ANOVA Results:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "print(\"\\nInterpretation:\")\n",
    "if anova_table['PR(>F)']['C(Software)'] < alpha:\n",
    "    print(\"There is a significant main effect of Software (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Software (p >= 0.05).\")\n",
    "\n",
    "if anova_table['PR(>F)']['C(Experience)'] < alpha:\n",
    "    print(\"There is a significant main effect of Experience (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of Experience (p >= 0.05).\")\n",
    "\n",
    "if anova_table['PR(>F)']['C(Software):C(Experience)'] < alpha:\n",
    "    print(\"There is a significant interaction effect between Software and Experience (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between Software and Experience (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76793996-8a89-4ea8-aa82-831140e8196e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
